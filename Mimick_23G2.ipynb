{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cszll7WVX5og"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hmWaDU6aqyR"
      },
      "outputs": [],
      "source": [
        "import smart_open\n",
        "import gensim.downloader\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3rSw4oEYBZO",
        "outputId": "5dbb58b1-6c1a-435b-f94f-3dcabce4ea52"
      },
      "outputs": [],
      "source": [
        "wandb.login(key='f8dba7836d4d8c528b40ebd197a992eb44f9c29f')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vrOHykBbgOL"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yelchdfj3pQa"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "from polyglot.mapping import Embedding\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from polyglot.downloader import downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yp7EXZYO31_e"
      },
      "outputs": [],
      "source": [
        "#@title Word_embedding\n",
        "class Word_embedding:\n",
        "    def __init__(self, emb_dim=300, w2v_size=50000, lang='en', embedding='polyglot'):\n",
        "        '''\n",
        "        Initializing word embedding\n",
        "        Parameter:\n",
        "        emb_dim = (int) embedding dimension for word embedding\n",
        "        '''\n",
        "        if embedding == 'glove':\n",
        "            # *GloVE\n",
        "            glove = GloVe('6B', dim=emb_dim)\n",
        "            self.embedding_vectors = glove.vectors\n",
        "            self.stoi = glove.stoi\n",
        "            self.itos = glove.itos\n",
        "        elif embedding == 'word2vec':\n",
        "            # *word2vec\n",
        "            # word2vec = Vectors('GoogleNews-vectors-negative300.bin.gz.txt')\n",
        "            w2v_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
        "            self.embedding_vectors = torch.from_numpy(w2v_vectors.vectors[:w2v_size])\n",
        "            self.stoi = w2v_vectors.key_to_index\n",
        "            self.itos = w2v_vectors.index_to_key[:w2v_size]\n",
        "        elif embedding == 'polyglot':\n",
        "            # *Polyglot\n",
        "            downloader.download(\"embeddings2.en\", download_dir='polyglot')\n",
        "            polyglot_emb = Embedding.load('polyglot/embeddings2/%s/embeddings_pkl.tar.bz2' % lang)\n",
        "            self.embedding_vectors = torch.from_numpy(polyglot_emb.vectors)\n",
        "            self.stoi = polyglot_emb.vocabulary.word_id\n",
        "            self.itos = [polyglot_emb.vocabulary.id_word[i] for i in range(len(polyglot_emb.vocabulary.id_word))]\n",
        "        elif embedding == 'dict2vec':\n",
        "            if not os.path.exists('dict2vec-100d.vec'):\n",
        "                !wget -O dict2vec-100d.vec https://raw.githubusercontent.com/yonathansantosa/Mimick/master/dict2vec-100d.vec\n",
        "            word2vec = Vectors('dict2vec-100d.vec')\n",
        "            self.embedding_vectors = word2vec.vectors\n",
        "            self.stoi = word2vec.stoi\n",
        "            self.itos = word2vec.itos\n",
        "        self.word_embedding = nn.Embedding.from_pretrained(self.embedding_vectors, freeze=True, sparse=True)\n",
        "        self.emb_dim = self.embedding_vectors.size(1)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (torch.tensor([index], dtype=torch.long), self.word_embedding(torch.tensor([index])).squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def update_weight(self, weight):\n",
        "        new_emb = Vectors(weight)\n",
        "        self.embedding_vectors = new_emb.vectors\n",
        "        self.word_embedding = nn.Embedding.from_pretrained(self.embedding_vectors, freeze=True, sparse=True)\n",
        "        self.emb_dim = self.embedding_vectors.size(1)\n",
        "        self.stoi = new_emb.stoi\n",
        "        self.itos = new_emb.itos\n",
        "\n",
        "    def word2idx(self, c):\n",
        "        return self.stoi[c]\n",
        "\n",
        "    def idx2word(self, idx):\n",
        "        return self.itos[int(idx)]\n",
        "\n",
        "    def idxs2sentence(self, idxs):\n",
        "        return ' '.join([self.itos[int(i)] for i in idxs])\n",
        "\n",
        "    def sentence2idxs(self, sentence):\n",
        "        word = sentence.split()\n",
        "        return [self.stoi[w] for w in word]\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        '''\n",
        "        Return tensor of indexes as a sentence\n",
        "\n",
        "        Input:\n",
        "        idxs = (torch.LongTensor) 1D tensor contains indexes\n",
        "        '''\n",
        "        idxs = idxs.squeeze()\n",
        "        sentence = [self.itos[int(idx)] for idx in idxs]\n",
        "        return sentence\n",
        "\n",
        "    def get_word_vectors(self):\n",
        "        return self.word_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HE5kO6EIaInk"
      },
      "outputs": [],
      "source": [
        "#@title Word_embedding_test\n",
        "class Word_embedding_test:\n",
        "    def __init__(self, emb_dim=300):\n",
        "        '''\n",
        "        Initializing word embedding\n",
        "        Parameter:\n",
        "        emb_dim = (int) embedding dimension for word embedding\n",
        "        '''\n",
        "        self.embedding = './.vector_cache/GoogleNews-vectors-negative300.bin.gz-'\n",
        "        self.stoi = \"./.vector_cache/stoi.txt\"\n",
        "        self.emb_dim = 0\n",
        "\n",
        "        with open('%s%d.txt' % (self.embedding, 0), encoding='utf-8') as fp:\n",
        "            for line in fp:\n",
        "                entry = line.split(' ')\n",
        "                self.emb_dim = len(entry) - 1\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_id = index // 100000\n",
        "        file_line = index % 100000\n",
        "        vector = None\n",
        "        with open('%s%d.txt' % (self.embedding, file_id), encoding='utf-8') as fp:\n",
        "            for i, line in enumerate(fp):\n",
        "                if i == file_line:\n",
        "                    entry = line.split(' ')\n",
        "                    vector = np.array(entry[1:], dtype=np.float32)\n",
        "                elif i > file_line:\n",
        "                    break\n",
        "        return (torch.tensor([file_id, file_line], dtype=torch.long), torch.tensor(vector))\n",
        "\n",
        "    def __len__(self):\n",
        "        with open(self.stoi, encoding='utf-8') as f:\n",
        "            for i, l in enumerate(f):\n",
        "                pass\n",
        "        return i + 1\n",
        "\n",
        "    def idx2word(self, file_idx, file_line):\n",
        "        with open('%s%d.txt' % (self.embedding, file_idx), encoding='utf-8') as fp:\n",
        "            for i, line in enumerate(fp):\n",
        "                if i == file_line:\n",
        "                    entry = line.split(' ')\n",
        "                    return(entry[0])\n",
        "                elif i > file_line:\n",
        "                    break\n",
        "\n",
        "    # def idxs2sentence(self, idxs):\n",
        "    #     return ' '.join([self.itos[int(i)] for i in idxs])\n",
        "\n",
        "    # def sentence2idxs(self, sentence):\n",
        "    #     word = sentence.split()\n",
        "    #     return [self.stoi[w] for w in word]\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        '''\n",
        "        Return tensor of indexes as a sentence\n",
        "\n",
        "        Input:\n",
        "        idxs = (torch.LongTensor) 1D tensor contains indexes\n",
        "        '''\n",
        "        return [self.idx2word(idx, line) for idx, line in idxs]\n",
        "\n",
        "    # def get_word_vectors(self):\n",
        "    #     return self.word_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIdSYutGblKh"
      },
      "outputs": [],
      "source": [
        "#@title Char Embedding\n",
        "class Char_embedding:\n",
        "    def __init__(self, char_emb_dim=300, char_max_len=15, random=False, asc=False, device='cuda', freeze=False):\n",
        "        super(Char_embedding, self).__init__()\n",
        "        '''\n",
        "        Initializing character embedding\n",
        "        Parameter:\n",
        "        emb_dim = (int) embedding dimension for character embedding\n",
        "        ascii = mutually exclusive with random\n",
        "        '''\n",
        "        self.char_max_len = char_max_len\n",
        "        self.asc = asc\n",
        "        if random and not self.asc:\n",
        "            torch.manual_seed(5)\n",
        "            if not os.path.exists('glove.840B.300d-char.txt'):\n",
        "                !wget -O glove.840B.300d-char.txt https://raw.githubusercontent.com/yonathansantosa/Mimick/master/glove.840B.300d-char.txt\n",
        "            table = np.transpose(np.loadtxt('glove.840B.300d-char.txt', dtype=str, delimiter=' ', comments='##'))\n",
        "            self.weight_char = np.transpose(table[1:].astype(float))\n",
        "            self.char = np.transpose(table[0])\n",
        "            self.embed = nn.Embedding(len(self.char), char_emb_dim).to(device)\n",
        "            None\n",
        "        elif self.asc:\n",
        "            if not os.path.exists('ascii.embedding.txt'):\n",
        "                !wget -O ascii.embedding.txt https://raw.githubusercontent.com/yonathansantosa/Mimick/master/ascii.embedding.txt\n",
        "            table = np.transpose(np.loadtxt('ascii.embedding.txt', dtype=str, delimiter=' ', comments='##'))\n",
        "            self.char = np.transpose(table[0])\n",
        "            self.weight_char = np.transpose(table[1:].astype(float))\n",
        "\n",
        "            self.weight_char = torch.from_numpy(self.weight_char).to(device)\n",
        "\n",
        "            self.embed = nn.Embedding.from_pretrained(self.weight_char, freeze=freeze)\n",
        "        else:\n",
        "            if not os.path.exists('glove.840B.300d-char.txt'):\n",
        "                !wget -O glove.840B.300d-char.txt https://raw.githubusercontent.com/yonathansantosa/Mimick/master/glove.840B.300d-char.txt\n",
        "            table = np.transpose(np.loadtxt('glove.840B.300d-char.txt', dtype=str, delimiter=' ', comments='##'))\n",
        "            self.char = np.transpose(table[0])\n",
        "            self.weight_char = np.transpose(table[1:].astype(float))\n",
        "            self.weight_char = self.weight_char[:,:char_emb_dim]\n",
        "\n",
        "            self.weight_char = torch.from_numpy(self.weight_char).to(device)\n",
        "\n",
        "            self.embed = nn.Embedding.from_pretrained(self.weight_char, freeze=freeze)\n",
        "\n",
        "        self.embed.padding_idx = 1\n",
        "        self.char2idx = {}\n",
        "        self.idx2char = {}\n",
        "        self.char_emb_dim = char_emb_dim\n",
        "        for i, c in enumerate(self.char):\n",
        "            self.char2idx[c] = int(i)\n",
        "            self.idx2char[i] = c\n",
        "\n",
        "    def char_split(self, sentence, dropout=0.):\n",
        "        '''\n",
        "        Splitting character of a sentences then converting it\n",
        "        into list of index\n",
        "\n",
        "        Parameter:\n",
        "\n",
        "        sentence = list of words\n",
        "        '''\n",
        "        char_data = []\n",
        "        numbers = set(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'])\n",
        "        # split_sentence = sentence.split()\n",
        "        # split_sentence = sentence.split()\n",
        "\n",
        "        for word in sentence:\n",
        "            if word == '<pad>':\n",
        "                char_data += [[self.char2idx['<pad>']] * self.char_max_len]\n",
        "            else:\n",
        "                c = list(word)\n",
        "                c = ['<sow>'] + c\n",
        "                if len(c) > self.char_max_len:\n",
        "                    # c_idx = [self.char2idx['#'] if x in numbers else self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c[:self.char_max_len]]\n",
        "                    c_idx = [self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c[:self.char_max_len]]\n",
        "                elif len(c) <= self.char_max_len:\n",
        "                    # c_idx = [self.char2idx['#'] if x in numbers else self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c]\n",
        "                    c_idx = [self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c]\n",
        "                    if len(c_idx) < self.char_max_len: c_idx.append(self.char2idx['<eow>'])\n",
        "                    for i in range(self.char_max_len-len(c)-1):\n",
        "                        c_idx.append(self.char2idx['<pad>'])\n",
        "                char_data += [c_idx]\n",
        "\n",
        "        char_data = torch.Tensor(char_data).long()\n",
        "        char_data = F.dropout(char_data, dropout)\n",
        "        return char_data\n",
        "\n",
        "    def char_sents_split(self, sentences, dropout=0.):\n",
        "        '''\n",
        "        Splitting character of a sentences then converting it\n",
        "        into list of index\n",
        "\n",
        "        Parameter:\n",
        "\n",
        "        sentence = list of words\n",
        "        '''\n",
        "        numbers = set(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'])\n",
        "        # split_sentence = sentence.split()\n",
        "        # split_sentence = sentence.split()\n",
        "\n",
        "        sents_data = []\n",
        "        for sentence in sentences:\n",
        "            char_data = []\n",
        "            for word in sentence:\n",
        "                if word == '<pad>':\n",
        "                    char_data += [[self.char2idx['<pad>']] * self.char_max_len]\n",
        "                else:\n",
        "                    c = list(word)\n",
        "                    c = ['<sow>'] + c\n",
        "                    if len(c) > self.char_max_len:\n",
        "                        # c_idx = [self.char2idx['#'] if x in numbers else self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c[:self.char_max_len]]\n",
        "                        c_idx = [self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c[:self.char_max_len]]\n",
        "                    elif len(c) <= self.char_max_len:\n",
        "                        # c_idx = [self.char2idx['#'] if x in numbers else self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c]\n",
        "                        c_idx = [self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c]\n",
        "                        if len(c_idx) < self.char_max_len: c_idx.append(self.char2idx['<eow>'])\n",
        "                        for i in range(self.char_max_len-len(c)-1):\n",
        "                            c_idx.append(self.char2idx['<pad>'])\n",
        "                    char_data += [c_idx]\n",
        "\n",
        "            char_data = torch.Tensor(char_data).long()\n",
        "            char_data = F.dropout(char_data, dropout)\n",
        "            sents_data += [char_data]\n",
        "\n",
        "        return torch.cat(sents_data)\n",
        "\n",
        "    def char2ix(self, c):\n",
        "        return self.char2idx[c]\n",
        "\n",
        "    def ix2char(self, idx):\n",
        "        return self.idx2char[idx]\n",
        "\n",
        "    def idxs2word(self, idxs):\n",
        "        return \"\".join([self.idx2char[idx] for idx in idxs])\n",
        "\n",
        "    def word2idxs(self, word):\n",
        "        char_data = []\n",
        "        if word != '<pad>':\n",
        "            chars = list(word)\n",
        "            chars = ['<sow>'] + chars\n",
        "            if len(chars) > self.char_max_len:\n",
        "                # c_idx = [self.char2idx['#'] if x in numbers else self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c[:self.char_max_len]]\n",
        "                c_idx = [self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in chars[:self.char_max_len]]\n",
        "            elif len(chars) <= self.char_max_len:\n",
        "                # c_idx = [self.char2idx['#'] if x in numbers else self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in c]\n",
        "                c_idx = [self.char2idx[x] if x in self.char2idx else self.char2idx['<unk>'] for x in chars]\n",
        "                if len(c_idx) < self.char_max_len: c_idx.append(self.char2idx['<eow>'])\n",
        "                for i in range(self.char_max_len-len(chars)-1):\n",
        "                    c_idx.append(self.char2idx['<pad>'])\n",
        "        else:\n",
        "            c_idx = [self.char2idx['<pad>']] * self.char_max_len\n",
        "\n",
        "        char_data += c_idx\n",
        "\n",
        "        return torch.LongTensor(char_data)\n",
        "\n",
        "    def clean_idxs2word(self, idxs):\n",
        "        idxs = [i for i in idxs if i != 0 and i != 1 and i != 2 and i != 3]\n",
        "        return \"\".join([self.idx2char[idx] for idx in idxs])\n",
        "\n",
        "    def get_char_vectors(self, words):\n",
        "        sentence = []\n",
        "        for idxs in words:\n",
        "            sentence += [self.char_embedding(idxs)]\n",
        "\n",
        "        # return torch.unsqueeze(torch.stack(sentence), 1).permute(1, 0, 2)\n",
        "        return torch.stack(sentence).permute(1, 0, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNKyv3hWdWeh"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8VD4Nnn06jp7"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GoBlbtrYdXMv"
      },
      "outputs": [],
      "source": [
        "#@title Models\n",
        "# *Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class mimick(nn.Module):\n",
        "    def __init__(self, embedding, char_emb_dim,emb_dim, hidden_size):\n",
        "        super(mimick, self).__init__()\n",
        "        self.embedding = nn.Embedding(embedding.num_embeddings, embedding.embedding_dim)\n",
        "        self.embedding.weight.data.copy_(embedding.weight.data)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(char_emb_dim, self.hidden_size, 1, bidirectional=True, batch_first=True)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size, 300),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(300, emb_dim),\n",
        "            nn.Hardtanh(min_val=-3.0, max_val=3.0),\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.embedding(inputs).float()\n",
        "        _, (hidden_state, _) = self.lstm(x)\n",
        "        out_cat = (hidden_state[0, :, :] + hidden_state[1, :, :])\n",
        "        out = self.mlp(out_cat)\n",
        "\n",
        "        return out\n",
        "\n",
        "class mimick_cnn(nn.Module):\n",
        "    def __init__(self, embedding,  char_max_len=15, char_emb_dim=300, emb_dim=300, num_feature=100, random=False, asc=False):\n",
        "        super(mimick_cnn, self).__init__()\n",
        "        self.embedding = nn.Embedding(embedding.num_embeddings, embedding.embedding_dim)\n",
        "        self.embedding.weight.data.copy_(embedding.weight.data)\n",
        "        self.conv2 = nn.Conv2d(1, num_feature, (2, char_emb_dim), bias=False)\n",
        "        self.conv3 = nn.Conv2d(1, num_feature, (3, char_emb_dim), bias=False)\n",
        "        self.conv4 = nn.Conv2d(1, num_feature, (4, char_emb_dim), bias=False)\n",
        "        self.conv5 = nn.Conv2d(1, num_feature, (5, char_emb_dim), bias=False)\n",
        "        self.conv6 = nn.Conv2d(1, num_feature, (6, char_emb_dim), bias=False)\n",
        "        self.conv7 = nn.Conv2d(1, num_feature, (7, char_emb_dim), bias=False)\n",
        "        self.inputs = None\n",
        "\n",
        "        self.mlp1 = nn.Sequential(\n",
        "            nn.Linear(num_feature*6, emb_dim),\n",
        "            nn.Hardtanh(min_val=-3.0, max_val=3.0),\n",
        "            # nn.Linear(400, 300),\n",
        "            # nn.Hardtanh()\n",
        "        )\n",
        "\n",
        "        self.mlp2 = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Hardtanh(min_val=-3.0, max_val=3.0),\n",
        "            # nn.Linear(400, 300),\n",
        "            # nn.Hardtanh()\n",
        "        )\n",
        "\n",
        "        self.t = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        x = self.embedding(self.inputs).float()\n",
        "        x2 = self.conv2(x).relu().squeeze(-1)\n",
        "        x3 = self.conv3(x).relu().squeeze(-1)\n",
        "        x4 = self.conv4(x).relu().squeeze(-1)\n",
        "        x5 = self.conv5(x).relu().squeeze(-1)\n",
        "        x6 = self.conv6(x).relu().squeeze(-1)\n",
        "        x7 = self.conv7(x).relu().squeeze(-1)\n",
        "\n",
        "\n",
        "        x2_max = F.max_pool1d(x2, x2.shape[2]).squeeze(-1)\n",
        "        x3_max = F.max_pool1d(x3, x3.shape[2]).squeeze(-1)\n",
        "        x4_max = F.max_pool1d(x4, x4.shape[2]).squeeze(-1)\n",
        "        x5_max = F.max_pool1d(x5, x5.shape[2]).squeeze(-1)\n",
        "        x6_max = F.max_pool1d(x6, x6.shape[2]).squeeze(-1)\n",
        "        x7_max = F.max_pool1d(x7, x7.shape[2]).squeeze(-1)\n",
        "\n",
        "\n",
        "        maxpoolcat = torch.cat([x2_max, x3_max, x4_max, x5_max, x6_max, x7_max], dim=1)\n",
        "\n",
        "        out_cnn = self.mlp1(maxpoolcat)\n",
        "\n",
        "        out = self.t(out_cnn) * self.mlp2(out_cnn) + (1 - self.t(out_cnn)) * out_cnn\n",
        "\n",
        "        return out\n",
        "\n",
        "class mimick_cnn2(nn.Module):\n",
        "    def __init__(self, embedding,  char_max_len=15, char_emb_dim=300, emb_dim=300, num_feature=100, random=False, asc=False):\n",
        "        super(mimick_cnn2, self).__init__()\n",
        "        self.embedding = nn.Embedding(embedding.num_embeddings, embedding.embedding_dim)\n",
        "        self.embedding.weight.data.copy_(embedding.weight.data)\n",
        "        self.conv1 = nn.Conv2d(1, num_feature, (2, char_emb_dim))\n",
        "        self.conv2 = nn.Conv1d(num_feature, num_feature, 2)\n",
        "        self.conv3 = nn.Conv1d(num_feature, emb_dim, 2)\n",
        "        self.conv4 = nn.Conv1d(emb_dim, emb_dim, 2)\n",
        "\n",
        "\n",
        "        self.mlp1 = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Hardtanh(min_val=-3.0, max_val=3.0),\n",
        "            # nn.Linear(400, 300),\n",
        "            # nn.Hardtanh()\n",
        "        )\n",
        "\n",
        "        self.mlp2 = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Hardtanh(min_val=-3.0, max_val=3.0),\n",
        "            # nn.Linear(400, 300),\n",
        "            # nn.Hardtanh()\n",
        "        )\n",
        "\n",
        "        self.t = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.embedding(inputs).float()\n",
        "        x2_conv1 = self.conv1(x).relu().squeeze(-1)\n",
        "        x2_max1 = F.max_pool1d(x2_conv1, 2).squeeze(-1)\n",
        "        x2_conv2 = self.conv2(x2_max1).relu()\n",
        "        x2_max2 = F.max_pool1d(x2_conv2, 2)\n",
        "        x2_conv3 = self.conv3(x2_max2).relu()\n",
        "        x2_max3 = F.max_pool1d(x2_conv3, x2_conv3.shape[2]).squeeze(-1)\n",
        "\n",
        "        # maxpoolcat = torch.cat([x2_max, x3_max, x4_max, x5_max, x6_max, x7_max], dim=2).view(inputs.size(0), -1)\n",
        "\n",
        "        out_cnn = self.mlp1(x2_max3)\n",
        "\n",
        "        out = self.t(out_cnn) * self.mlp2(out_cnn) + (1 - self.t(out_cnn)) * out_cnn\n",
        "\n",
        "        return out\n",
        "\n",
        "class mimick_cnn3(nn.Module):\n",
        "    def __init__(self, embedding, char_max_len=15, char_emb_dim=300, emb_dim=300, num_feature=100, mtp=1, random=False, asc=False):\n",
        "        super(mimick_cnn3, self).__init__()\n",
        "        self.embedding = nn.Embedding(embedding.num_embeddings, embedding.embedding_dim)\n",
        "        self.embedding.weight.data.copy_(embedding.weight.data)\n",
        "        self.conv2 = nn.Conv2d(1, num_feature, (2, embedding.embedding_dim), bias=False)\n",
        "        self.conv3 = nn.Conv2d(1, num_feature, (3, embedding.embedding_dim), bias=False)\n",
        "        self.conv4 = nn.Conv2d(1, num_feature, (4, embedding.embedding_dim), bias=False)\n",
        "        self.conv5 = nn.Conv2d(1, num_feature, (5, embedding.embedding_dim), bias=False)\n",
        "        self.conv6 = nn.Conv2d(1, num_feature, (6, embedding.embedding_dim), bias=False)\n",
        "        self.conv7 = nn.Conv2d(1, num_feature, (7, embedding.embedding_dim), bias=False)\n",
        "\n",
        "        self.featloc = nn.Sequential(\n",
        "            nn.Linear(num_feature*99, emb_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.mlp1 = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Hardtanh(min_val=-mtp*3, max_val=mtp*3),\n",
        "            # nn.Linear(400, 300),\n",
        "            # nn.Hardtanh()\n",
        "        )\n",
        "\n",
        "        self.mlp2 = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Hardtanh(min_val=-mtp*3, max_val=mtp*3),\n",
        "            # nn.Linear(400, 300),\n",
        "            # nn.Hardtanh()\n",
        "        )\n",
        "\n",
        "        self.t = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.embedding(inputs).float()\n",
        "        x2 = self.conv2(x).sigmoid().squeeze(-1)\n",
        "        x3 = self.conv3(x).sigmoid().squeeze(-1)\n",
        "        x4 = self.conv4(x).sigmoid().squeeze(-1)\n",
        "        x5 = self.conv5(x).sigmoid().squeeze(-1)\n",
        "        x6 = self.conv6(x).sigmoid().squeeze(-1)\n",
        "        x7 = self.conv7(x).sigmoid().squeeze(-1)\n",
        "\n",
        "        x2 = x2.view(x2.shape[0], -1)\n",
        "        x3 = x3.view(x3.shape[0], -1)\n",
        "        x4 = x4.view(x4.shape[0], -1)\n",
        "        x5 = x5.view(x5.shape[0], -1)\n",
        "        x6 = x6.view(x6.shape[0], -1)\n",
        "        x7 = x7.view(x7.shape[0], -1)\n",
        "\n",
        "        concat = torch.cat([x2,x3,x4,x5,x6,x7], dim=1)\n",
        "\n",
        "        feature_loc = self.featloc(concat)\n",
        "\n",
        "        out_cnn = self.mlp1(feature_loc)\n",
        "\n",
        "        out = self.t(out_cnn) * self.mlp2(out_cnn) + (1 - self.t(out_cnn)) * out_cnn\n",
        "\n",
        "        return out\n",
        "\n",
        "class mimick_cnn4(nn.Module):\n",
        "    def __init__(self, embedding, char_max_len=15, char_emb_dim=300, emb_dim=300, num_feature=100, classif=200, random=False, asc=False):\n",
        "        super(mimick_cnn4, self).__init__()\n",
        "        self.embedding = nn.Embedding(embedding.num_embeddings, embedding.embedding_dim)\n",
        "        self.embedding.weight.data.copy_(embedding.weight.data)\n",
        "        self.conv2 = nn.Conv2d(1, num_feature, (2, char_emb_dim))\n",
        "        self.conv3 = nn.Conv2d(1, num_feature, (3, char_emb_dim))\n",
        "        self.conv4 = nn.Conv2d(1, num_feature, (4, char_emb_dim))\n",
        "        self.conv5 = nn.Conv2d(1, num_feature, (5, char_emb_dim))\n",
        "        self.conv6 = nn.Conv2d(1, num_feature, (6, char_emb_dim))\n",
        "        self.conv7 = nn.Conv2d(1, num_feature, (7, char_emb_dim))\n",
        "\n",
        "        self.classif = nn.Sequential(\n",
        "            nn.Linear(num_feature*48, classif),\n",
        "            nn.LogSoftmax()\n",
        "        )\n",
        "\n",
        "        self.regres = nn.Sequential(\n",
        "            nn.Linear(classif, emb_dim),\n",
        "            nn.Hardtanh(min_val=-3, max_val=3)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x2 = self.conv2(inputs).relu().squeeze(-1)\n",
        "        x3 = self.conv3(inputs).relu().squeeze(-1)\n",
        "        x4 = self.conv4(inputs).relu().squeeze(-1)\n",
        "        x5 = self.conv5(inputs).relu().squeeze(-1)\n",
        "        x6 = self.conv6(inputs).relu().squeeze(-1)\n",
        "        x7 = self.conv7(inputs).relu().squeeze(-1)\n",
        "\n",
        "\n",
        "        x2_max = F.max_pool1d(x2, 2).squeeze(-1)\n",
        "        x3_max = F.max_pool1d(x3, 2).squeeze(-1)\n",
        "        x4_max = F.max_pool1d(x4, 2).squeeze(-1)\n",
        "        x5_max = F.max_pool1d(x5, 2).squeeze(-1)\n",
        "        x6_max = F.max_pool1d(x6, 2).squeeze(-1)\n",
        "        x7_max = F.max_pool1d(x7, 2).squeeze(-1)\n",
        "\n",
        "\n",
        "        maxpoolcat = torch.cat([x2_max, x3_max, x4_max, x5_max, x6_max, x7_max], dim=2).view(inputs.size(0), -1)\n",
        "\n",
        "        c = self.classif(maxpoolcat)\n",
        "\n",
        "        out = self.regres(c)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLDQgCA1db8x"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KyYS-1bhdzvL"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(\n",
        "    description='Conditional Text Generation: Train Discriminator'\n",
        ")\n",
        "\n",
        "parser.add_argument('--maxepoch', default=30, help='maximum iteration (default=1000)')\n",
        "parser.add_argument('--run', default=1, help='starting epoch (default=1)')\n",
        "parser.add_argument('--save', default=False, action='store_true', help='whether to save model or not')\n",
        "parser.add_argument('--load', default=False, action='store_true', help='whether to load model or not')\n",
        "parser.add_argument('--lang', default='en', help='choose which language for word embedding')\n",
        "parser.add_argument('--model', default='lstm', help='choose which mimick model')\n",
        "parser.add_argument('--lr', default=0.1, help='learning rate')\n",
        "parser.add_argument('--charlen', default=20, help='maximum length')\n",
        "parser.add_argument('--charembdim', default=300)\n",
        "parser.add_argument('--embedding', default='polyglot')\n",
        "parser.add_argument('--local', default=False, action='store_true')\n",
        "parser.add_argument('--loss_fn', default='mse')\n",
        "parser.add_argument('--dropout', default=0)\n",
        "parser.add_argument('--bsize', default=64)\n",
        "parser.add_argument('--epoch', default=0)\n",
        "parser.add_argument('--asc', default=False, action='store_true')\n",
        "parser.add_argument('--quiet', default=False, action='store_true')\n",
        "parser.add_argument('--init_weight', default=False, action='store_true')\n",
        "parser.add_argument('--shuffle', default=False, action='store_true')\n",
        "parser.add_argument('--nesterov', default=False, action='store_true')\n",
        "parser.add_argument('--loss_reduction', default=False, action='store_true')\n",
        "parser.add_argument('--num_feature', default=50)\n",
        "parser.add_argument('--weight_decay', default=0)\n",
        "parser.add_argument('--momentum', default=0)\n",
        "parser.add_argument('--multiplier', default=1)\n",
        "parser.add_argument('--classif', default=200)\n",
        "parser.add_argument('--neighbor', default=5)\n",
        "parser.add_argument('--seed', default=64)\n",
        "\n",
        "max_epoch = 100\n",
        "run = 1\n",
        "save = True\n",
        "load = False\n",
        "lang = \"en\"\n",
        "model = \"lstm\"\n",
        "lr = 0.1\n",
        "charlen = 20\n",
        "charembdim = 300\n",
        "embedding = \"word2vec\"\n",
        "local = True\n",
        "loss_fn = \"mse\"\n",
        "dropout = 0\n",
        "bsize = 64\n",
        "epoch = 0\n",
        "asc = False\n",
        "quiet = False\n",
        "init_weight = False\n",
        "shuffle = False\n",
        "nesterov = False\n",
        "loss_reduction = False\n",
        "num_feature = 50\n",
        "weight_decay = 0\n",
        "momentum = 0\n",
        "multiplier = 1\n",
        "classif = 200\n",
        "neighbor = 5\n",
        "seed = 64\n",
        "\n",
        "argmnt = [\n",
        "    '--maxepoch', f'{max_epoch}',\n",
        "    '--run', f'{run}',\n",
        "    '--lang', f'{lang}',\n",
        "    '--model', f'{model}',\n",
        "    '--lr', f'{lr}',\n",
        "    '--charlen', f'{charlen}',\n",
        "    '--charembdim', f'{charembdim}',\n",
        "    '--embedding', f'{embedding}',\n",
        "    '--loss_fn', f'{loss_fn}',\n",
        "    '--dropout', f'{dropout}',\n",
        "    '--bsize', f'{bsize}',\n",
        "    '--epoch', f'{epoch}',\n",
        "    '--num_feature', f'{num_feature}',\n",
        "    '--weight_decay', f'{weight_decay}',\n",
        "    '--momentum', f'{momentum}',\n",
        "    '--multiplier', f'{multiplier}',\n",
        "    '--classif', f'{classif}',\n",
        "    '--neighbor', f'{neighbor}',\n",
        "    '--seed', f'{seed}',\n",
        "]\n",
        "if local: argmnt += ['--local']\n",
        "if asc: argmnt += ['--asc']\n",
        "if quiet: argmnt += ['--quiet']\n",
        "if init_weight: argmnt += ['--init_weight']\n",
        "if shuffle: argmnt += ['--shuffle']\n",
        "if nesterov: argmnt += ['--nesterov']\n",
        "if loss_reduction: argmnt += ['--loss_reduction']\n",
        "if save: argmnt += ['--save']\n",
        "if load: argmnt += ['--load']\n",
        "\n",
        "args = parser.parse_args(argmnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0bdfeb8d82cb4987ba1319908b49bf85",
            "f94e40ea772a4843b312661b5ec2e35a",
            "4b2b37ebdf524cfa9b76979cadba6248",
            "e444da69d34548ed9352cf35c74bcc9f",
            "314d6af552c3451db595d759ec78ba4b",
            "a2785f556f684fdc8a9da40c9d0041dd",
            "d50f7536c7cf482fb09e9e7803f9c836",
            "9f000672be1f4f57a340527bb738e089",
            "75c44389eb984e2eab4b674c62366900"
          ]
        },
        "id": "OrpSWpsgdbWA",
        "outputId": "c158402f-600c-4e3e-f524-ef5367c944c7"
      },
      "outputs": [],
      "source": [
        "#@title Training\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable, gradcheck\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime, timezone, timedelta\n",
        "timezone_offset = 7.0  # Pacific Standard Time (UTC−08:00)\n",
        "tzinfo = timezone(timedelta(hours=timezone_offset))\n",
        "curr_dt = datetime.now(tzinfo)\n",
        "# writer = SummaryWriter()\n",
        "wab_name = f\"{args.model}{args.embedding}{args.loss_fn}_{curr_dt.year-2000}{curr_dt.month:02}{curr_dt.day:02}{curr_dt.hour:02}{curr_dt.minute:02}\"\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"Mimick\",\n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=wab_name,\n",
        "    config={\n",
        "        \"run\":int(args.run),\n",
        "        \"char_emb_dim\":int(args.charembdim),\n",
        "        \"char_max_len\":int(args.charlen),\n",
        "        \"char_emb_ascii\":args.asc,\n",
        "        \"random_seed\":int(args.seed),\n",
        "        \"shuffle_dataset\":args.shuffle,\n",
        "        \"neighbor\":int(args.neighbor),\n",
        "        \"validation_split\":.8,\n",
        "        \"batch_size\":int(args.bsize),\n",
        "        \"max_epoch\":int(args.maxepoch),\n",
        "        \"learning_rate\":float(args.lr),\n",
        "        \"weight_decay\":float(args.weight_decay),\n",
        "        \"momentum\":float(args.momentum),\n",
        "        \"multiplier\":float(args.multiplier),\n",
        "        \"classif\":int(args.classif),\n",
        "        \"model\":args.model,\n",
        "        \"loss_fn\":args.loss_fn,\n",
        "        \"loss_reduction\":args.loss_reduction,\n",
        "    }\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# from model import *\n",
        "# from charembedding import Char_embedding\n",
        "# from wordembedding import Word_embedding\n",
        "\n",
        "# import argparse\n",
        "\n",
        "from tqdm import trange, tqdm\n",
        "import os\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "def cosine_similarity(tensor1, tensor2, neighbor=5):\n",
        "    '''\n",
        "    Calculating cosine similarity for each vector elements of\n",
        "    tensor 1 with each vector elements of tensor 2\n",
        "\n",
        "    Input:\n",
        "\n",
        "    tensor1 = (torch.FloatTensor) with size N x D\n",
        "    tensor2 = (torch.FloatTensor) with size M x D\n",
        "    neighbor = (int) number of closest vector to be returned\n",
        "\n",
        "    Output:\n",
        "\n",
        "    (distance, neighbor)\n",
        "    '''\n",
        "    tensor1_norm = torch.norm(tensor1, 2, 1)\n",
        "    tensor2_norm = torch.norm(tensor2, 2, 1)\n",
        "    tensor1_dot_tensor2 = torch.mm(tensor2, torch.t(tensor1)).t()\n",
        "\n",
        "    divisor = [t * tensor2_norm for t in tensor1_norm]\n",
        "\n",
        "    divisor = torch.stack(divisor)\n",
        "\n",
        "    # result = (tensor1_dot_tensor2/divisor).data.cpu().numpy()\n",
        "    result = (tensor1_dot_tensor2/divisor.clamp(min=1.e-09)).data.cpu()\n",
        "    d, n = torch.sort(result, descending=True)\n",
        "    n = n[:, :neighbor]\n",
        "    d = d[:, :neighbor]\n",
        "    return d, n\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "        m.weight.data.fill_(0.01)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "def pairwise_distances(x, y=None, multiplier=1., loss=False, neighbor=5):\n",
        "    '''\n",
        "    Input:\n",
        "\n",
        "    x is a Nxd matrix\n",
        "    y is an optional Mxd matirx\n",
        "\n",
        "    Output:\n",
        "\n",
        "    dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
        "    if y is not given then use 'y=x'.\n",
        "\n",
        "    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
        "    '''\n",
        "    x_norm = (x**2).sum(1).view(-1, 1)\n",
        "    if y is not None:\n",
        "        y *= multiplier\n",
        "        y_norm = (y**2).sum(1).view(1, -1)\n",
        "    else:\n",
        "        y = x\n",
        "        y_norm = x_norm.view(1, -1)\n",
        "    if loss:\n",
        "        result = F.pairwise_distance(x, y)\n",
        "        return result\n",
        "    else:\n",
        "        dist = x_norm + y_norm - 2.0 * torch.mm(x, torch.transpose(y, 0, 1))\n",
        "        d, n = torch.sort(dist, descending=False)\n",
        "        n = n[:, :neighbor]\n",
        "        d = d[:, :neighbor]\n",
        "        return d, n\n",
        "\n",
        "def decaying_alpha_beta(epoch=0, loss_fn='cosine'):\n",
        "    # decay = math.exp(-float(epoch)/200)\n",
        "    if loss_fn == 'cosine':\n",
        "        alpha = 1\n",
        "        beta = 0.5\n",
        "    else:\n",
        "        alpha = 0.5\n",
        "        beta = 1\n",
        "    return alpha, beta\n",
        "\n",
        "cloud_dir = '/content/gdrive/My Drive/train_dropout/'\n",
        "saved_model_path = 'trained_model_%s_%s_%s_%s' % (args.lang, args.model, args.embedding, args.loss_fn)\n",
        "logger_dir = '%s/logs/run%s/' % (saved_model_path, args.run)\n",
        "logger_val_dir = '%s/logs/val-run%s/' % (saved_model_path, args.run)\n",
        "\n",
        "if not args.local:\n",
        "    # logger_dir = cloud_dir + logger_dir\n",
        "    saved_model_path = cloud_dir + saved_model_path\n",
        "\n",
        "# *Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# *Parameters\n",
        "run = int(args.run)\n",
        "char_emb_dim = int(args.charembdim)\n",
        "char_max_len = int(args.charlen)\n",
        "random_seed = int(args.seed)\n",
        "shuffle_dataset = args.shuffle\n",
        "neighbor = int(args.neighbor)\n",
        "validation_split = .8\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# *Hyperparameter\n",
        "batch_size = int(args.bsize)\n",
        "max_epoch = int(args.maxepoch)\n",
        "learning_rate = float(args.lr)\n",
        "weight_decay = float(args.weight_decay)\n",
        "momentum = float(args.momentum)\n",
        "multiplier = float(args.multiplier)\n",
        "classif = int(args.classif)\n",
        "\n",
        "val_batch_size = 64\n",
        "\n",
        "char_embed = Char_embedding(char_emb_dim, char_max_len, asc=args.asc, random=True, device=device)\n",
        "\n",
        "dataset = Word_embedding(lang=args.lang, embedding=args.embedding)\n",
        "emb_dim = dataset.emb_dim\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "\n",
        "\n",
        "# if shuffle_dataset:\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "#* Creating PT data samplers and loaders:\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "np.random.shuffle(train_indices)\n",
        "np.random.shuffle(val_indices)\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size,\n",
        "                                sampler=train_sampler)\n",
        "validation_loader = DataLoader(dataset, batch_size=val_batch_size,\n",
        "                                sampler=valid_sampler)\n",
        "\n",
        "#* Initializing model\n",
        "if args.model == 'lstm':\n",
        "    model = mimick(char_embed.embed, char_embed.char_emb_dim, dataset.emb_dim, int(args.num_feature))\n",
        "elif args.model == 'cnn2':\n",
        "    model = mimick_cnn2(\n",
        "        embedding=char_embed.embed,\n",
        "        char_max_len=char_embed.char_max_len,\n",
        "        char_emb_dim=char_embed.char_emb_dim,\n",
        "        emb_dim=emb_dim,\n",
        "        num_feature=int(args.num_feature),\n",
        "        random=False, asc=args.asc)\n",
        "elif args.model == 'cnn':\n",
        "    model = mimick_cnn(\n",
        "        embedding=char_embed.embed,\n",
        "        char_max_len=char_embed.char_max_len,\n",
        "        char_emb_dim=char_embed.char_emb_dim,\n",
        "        emb_dim=emb_dim,\n",
        "        num_feature=int(args.num_feature),\n",
        "        random=False, asc=args.asc)\n",
        "elif args.model == 'cnn3':\n",
        "    model = mimick_cnn3(\n",
        "        embedding=char_embed.embed,\n",
        "        char_max_len=char_embed.char_max_len,\n",
        "        char_emb_dim=char_embed.char_emb_dim,\n",
        "        emb_dim=emb_dim,\n",
        "        num_feature=int(args.num_feature),\n",
        "        mtp=multiplier,\n",
        "        random=False, asc=args.asc)\n",
        "elif args.model == 'cnn4':\n",
        "    model = mimick_cnn4(\n",
        "        embedding=char_embed.embed,\n",
        "        char_max_len=char_embed.char_max_len,\n",
        "        char_emb_dim=char_embed.char_emb_dim,\n",
        "        emb_dim=emb_dim,\n",
        "        num_feature=int(args.num_feature),\n",
        "        classif=classif,\n",
        "        random=False, asc=args.asc)\n",
        "else:\n",
        "    model = None\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "if args.loss_fn == 'mse':\n",
        "    if args.loss_reduction:\n",
        "        criterion = nn.MSELoss(reduction='none')\n",
        "    else:\n",
        "        criterion = nn.MSELoss()\n",
        "else:\n",
        "    criterion = nn.CosineSimilarity()\n",
        "\n",
        "if run < 1:\n",
        "    args.run = '1'\n",
        "\n",
        "if run != 1:\n",
        "    args.load = True\n",
        "\n",
        "if args.load:\n",
        "    model.load_state_dict(torch.load('%s/%s.pth' % (saved_model_path, args.model)))\n",
        "\n",
        "elif not os.path.exists(saved_model_path):\n",
        "    os.makedirs(saved_model_path)\n",
        "\n",
        "word_embedding = dataset.embedding_vectors.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, nesterov=args.nesterov)\n",
        "\n",
        "if args.init_weight: model.apply(init_weights)\n",
        "\n",
        "step = 0\n",
        "# *Training\n",
        "\n",
        "for epoch in trange(int(args.epoch), max_epoch, total=max_epoch, initial=int(args.epoch)):\n",
        "    # conv2weight = model.conv2.weight.data.clone()\n",
        "    # mlpweight = model.mlp[2].weight.data.clone()\n",
        "    sum_loss = 0.\n",
        "    for it, (X, y) in enumerate(train_loader):\n",
        "        model.zero_grad()\n",
        "        words = dataset.idxs2words(X)\n",
        "        idxs = char_embed.char_split(words).to(device)\n",
        "        if args.model != 'lstm': idxs = idxs.unsqueeze(1)\n",
        "        inputs = Variable(idxs) # (length x batch x char_emb_dim)\n",
        "        target = Variable(y*multiplier).squeeze().to(device) # (batch x word_emb_dim)\n",
        "\n",
        "        output = model.forward(inputs) # (batch x word_emb_dim)\n",
        "        loss = criterion(output, target) if args.loss_fn == 'mse' else (1-criterion(output, target)).mean()\n",
        "\n",
        "        # ##################\n",
        "        # Tensorboard\n",
        "        # ##################\n",
        "        sum_loss += loss.item() if not args.loss_reduction else loss.mean().item()\n",
        "\n",
        "        if not args.loss_reduction:\n",
        "            loss.backward()\n",
        "        else:\n",
        "            loss = loss.mean(0)\n",
        "            for i in range(len(loss)-1):\n",
        "                loss[i].backward(retain_graph=True)\n",
        "\n",
        "            loss[len(loss)-1].backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if not args.quiet:\n",
        "            if it % int(dataset_size/(batch_size*5)) == 0:\n",
        "                tqdm.write('loss = %.4f' % loss.mean())\n",
        "                model.eval()\n",
        "                random_input = np.random.randint(len(X))\n",
        "\n",
        "                words = dataset.idx2word(X[random_input]) # list of words\n",
        "\n",
        "                distance, nearest_neighbor = cosine_similarity(output[random_input].detach().unsqueeze(0), word_embedding, neighbor=neighbor)\n",
        "\n",
        "                loss_dist = torch.dist(output[random_input], target[random_input]*multiplier)\n",
        "                tqdm.write('%d %.4f | ' % (step, loss_dist.item()) + words + '\\t=> ' + dataset.idxs2sentence(nearest_neighbor[0]))\n",
        "                model.train()\n",
        "                tqdm.write('')\n",
        "    # writer.add_scalar('Loss/train', sum_loss, epoch)\n",
        "    wandb.log({\"train_loss\": sum_loss / len(train_indices)})\n",
        "    model.eval()\n",
        "\n",
        "    ############################\n",
        "    # SAVING TRAINED MODEL\n",
        "    ############################\n",
        "\n",
        "    if not args.local:\n",
        "        copy_tree(logger_dir, cloud_dir+logger_dir)\n",
        "\n",
        "    torch.save(model.state_dict(), f'{saved_model_path}/{args.model}.pth')\n",
        "\n",
        "    val_loss = 0.\n",
        "    for it, (X, target) in enumerate(validation_loader):\n",
        "        words = dataset.idxs2words(X)\n",
        "        idxs = char_embed.char_split(words).to(device)\n",
        "        if args.model != 'lstm': idxs = idxs.unsqueeze(1)\n",
        "        inputs = Variable(idxs) # (length x batch x char_emb_dim)\n",
        "        target = target.to(device) # (batch x word_emb_dim)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        output = model.forward(inputs) # (batch x word_emb_dim)\n",
        "\n",
        "        loss = criterion(output, target) if args.loss_fn == 'mse' else (1-criterion(output, target)).mean()\n",
        "        val_loss += loss.item() if not args.loss_reduction else loss.mean().item()\n",
        "\n",
        "        if not args.quiet:\n",
        "            if it < 1:\n",
        "                distance, nearest_neighbor = cosine_similarity(output, word_embedding, neighbor=neighbor)\n",
        "\n",
        "                for i, word in enumerate(X):\n",
        "                    if i >= 1: break\n",
        "                    loss_dist = torch.dist(output[i], target[i])\n",
        "\n",
        "                    tqdm.write('%.4f | %s \\t=> %s' % (loss_dist.item(), dataset.idx2word(word), dataset.idxs2sentence(nearest_neighbor[i])))\n",
        "    # writer.add_scalar('Loss/Val', val_loss, epoch)\n",
        "    wandb.log({\"val_loss\": val_loss / len(val_indices)})\n",
        "\n",
        "    if not args.quiet: print('total loss = %.8f' % val_loss)\n",
        "\n",
        "#@title Testing Similarities\n",
        "model.eval()\n",
        "\n",
        "# *Evaluating\n",
        "words = 'MCT McNeally Vercellotti Secretive corssing flatfish compartmentalize pesky lawnmower developiong hurtling expectedly'.split()\n",
        "# words += args.words\n",
        "\n",
        "inputs = char_embed.char_split(words)\n",
        "\n",
        "embedding = dataset.embedding_vectors.to(device)\n",
        "inputs = inputs.to(device) # (length x batch x char_emb_dim)\n",
        "if args.model != 'lstm': inputs = inputs.unsqueeze(1)\n",
        "output = model.forward(inputs) # (batch x word_emb_dim)\n",
        "\n",
        "cos_dist, nearest_neighbor = cosine_similarity(output, embedding, neighbor)\n",
        "\n",
        "sim_table = []\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    sim_table += [[torch.mean(cos_dist[i]), word, dataset.idxs2sentence(nearest_neighbor[i])]]\n",
        "    # print('%.4f | ' % torch.mean(cos_dist[i]) + word + '\\t=> ' + dataset.idxs2sentence(nearest_neighbor[i]))\n",
        "my_table = wandb.Table(columns=[\"Cosine Similarity\", \"Word\", \"Similar Words\"], data=sim_table)\n",
        "wandb.log({\"Similarity Test\": my_table})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOlEUo19RC4j"
      },
      "source": [
        "# Downstream Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xycqWJtM96R"
      },
      "outputs": [],
      "source": [
        "#@title POStag Model\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "if not os.path.exists('brown.txt'):\n",
        "    !wget -O brown.txt https://raw.githubusercontent.com/yonathansantosa/Mimick/master/tagset/brown.txt\n",
        "if not os.path.exists('upenn.txt'):\n",
        "    !wget -O upenn.txt https://raw.githubusercontent.com/yonathansantosa/Mimick/master/tagset/upenn.txt\n",
        "np.random.seed(0)\n",
        "\n",
        "class Tagset:\n",
        "    def __init__(self, tagset='brown'):\n",
        "        self.itot = {}\n",
        "        self.toti = {}\n",
        "        with open ('%s.txt' % tagset, \"r\") as myfile:\n",
        "            data=myfile.readlines()\n",
        "            sent = \"\".join([d for d in data])\n",
        "            processed = re.findall(r\"(.*):\", sent)\n",
        "            for i, tag in enumerate(processed):\n",
        "                self.toti[tag] = i\n",
        "\n",
        "            for i, tag in enumerate(processed):\n",
        "                self.itot[i] = tag\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itot)\n",
        "\n",
        "    def idx2tag(self, idx):\n",
        "        return self.itot[idx]\n",
        "\n",
        "    def tag2idx(self, tag):\n",
        "        return self.toti[tag]\n",
        "\n",
        "\n",
        "class Postag:\n",
        "    def __init__(self, word_embed, corpus='brown', tagset='brown', device='cuda'):\n",
        "        if corpus == 'brown':\n",
        "            nltk.download('brown')\n",
        "            from nltk.corpus import brown as corpus\n",
        "        self.word_embed = word_embed\n",
        "        self.tagged_words = corpus.tagged_words(tagset=tagset)\n",
        "        self.tagged_sents = corpus.tagged_sents(tagset=tagset)\n",
        "        self.tagset = Tagset(tagset=tagset)\n",
        "        new_itot = {}\n",
        "        new_toti = {}\n",
        "        self.count_bin = torch.zeros(len(self.tagset))\n",
        "        self.idxs = torch.zeros(1)\n",
        "        self.device = device\n",
        "\n",
        "        for word, tag in self.tagged_words:\n",
        "            if tag in self.tagset.toti:\n",
        "                self.count_bin[self.tagset.tag2idx(tag)] += 1\n",
        "            else:\n",
        "                self.count_bin[self.tagset.tag2idx('UNK')] += 1\n",
        "\n",
        "        _, self.idxs = torch.sort(self.count_bin, descending=True)\n",
        "\n",
        "        for it, i in enumerate(self.idxs):\n",
        "            new_itot[it] = self.tagset.itot[int(i)]\n",
        "            new_toti[new_itot[it]] = it\n",
        "\n",
        "        self.tagset.toti = new_toti\n",
        "        self.tagset.itot = new_itot\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tagged_sents)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        length = len(self.tagged_sents[index])\n",
        "        word = []\n",
        "        tag = []\n",
        "\n",
        "        if length-5 <= 0:\n",
        "            for i in range(length):\n",
        "                w, t = self.tagged_sents[index][i]\n",
        "                word += [self.word_embed.word2idxs(w)]\n",
        "\n",
        "                if t in self.tagset.toti:\n",
        "                    tag_id = self.tagset.tag2idx(t)\n",
        "                else:\n",
        "                    tag_id = self.tagset.tag2idx('UNK')\n",
        "\n",
        "                tag += [tag_id]\n",
        "\n",
        "            for i in range(length, 5):\n",
        "                word += [self.word_embed.word2idxs('<pad>')]\n",
        "                tag_id = self.tagset.tag2idx('UNK')\n",
        "\n",
        "                tag += [tag_id]\n",
        "\n",
        "        else:\n",
        "            start_index = np.random.randint(0, length-5)\n",
        "            for i in range(start_index, start_index+5):\n",
        "                w, t = self.tagged_sents[index][i]\n",
        "                if t in self.tagset.toti:\n",
        "                    tag_id = self.tagset.tag2idx(t)\n",
        "                else:\n",
        "                    tag_id = self.tagset.tag2idx('UNK')\n",
        "                word += [self.word_embed.word2idxs(w)]\n",
        "                tag += [tag_id]\n",
        "\n",
        "        # word_emb = self.word_embed.word_embedding(torch.tensor(word).to(self.device))\n",
        "        # return (word_emb, torch.LongTensor(tag).view(len(tag), 1), torch.LongTensor(word).view(len(word), 1))\n",
        "        return (torch.vstack(word), torch.LongTensor(tag))\n",
        "\n",
        "class Postag_word:\n",
        "    def __init__(self, word_embed, char_embed, corpus='brown', tagset='brown'):\n",
        "        if corpus == 'brown':\n",
        "            nltk.download('brown')\n",
        "            from nltk.corpus import brown as corpus\n",
        "        self.char_embed = char_embed\n",
        "        self.tagged_words = corpus.tagged_words(tagset=tagset)\n",
        "        self.tagged_sents = corpus.tagged_sents(tagset=tagset)\n",
        "        self.tagset = Tagset(tagset=tagset)\n",
        "        new_itot = {}\n",
        "        new_toti = {}\n",
        "        self.word_embed = word_embed\n",
        "        self.count_bin = torch.zeros(len(self.tagset))\n",
        "        self.idxs = torch.zeros(1)\n",
        "\n",
        "        for _, tag in self.tagged_words:\n",
        "            if tag in self.tagset.toti:\n",
        "                self.count_bin[self.tagset.tag2idx(tag)] += 1\n",
        "            else:\n",
        "                self.count_bin[self.tagset.tag2idx('UNK')] += 1\n",
        "\n",
        "        _, self.idxs = torch.sort(self.count_bin, descending=True)\n",
        "\n",
        "        for it, i in enumerate(self.idxs):\n",
        "            new_itot[it] = self.tagset.itot[int(i)]\n",
        "            new_toti[new_itot[it]] = it\n",
        "\n",
        "        self.tagset.toti = new_toti\n",
        "        self.tagset.itot = new_itot\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tagged_sents)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        word, tag = self.tagged_words[index]\n",
        "\n",
        "        # if word in self.word_embed.stoi:\n",
        "        #     w_idx = self.word_embed.stoi[word]\n",
        "        # else:\n",
        "        #     w_idx = self.word_embed.stoi['</s>']\n",
        "\n",
        "        w_c_idx = self.char_embed.word2idxs(word)\n",
        "        if tag in self.tagset.toti:\n",
        "            tag_id = self.tagset.tag2idx(tag)\n",
        "        else:\n",
        "            tag_id = self.tagset.tag2idx('UNK')\n",
        "\n",
        "        try:\n",
        "            w_idx = self.word_embed.stoi[word]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return (torch.LongTensor(w_idx), torch.LongTensor(w_c_idx), torch.LongTensor(tag_id))\n",
        "\n",
        "class Postagger(nn.Module):\n",
        "    def __init__(self, seq_length, emb_dim, hidden_size, output_size):\n",
        "        super(Postagger, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.seq_length = seq_length\n",
        "        self.lstm = nn.LSTM(emb_dim, self.hidden_size, 1, bidirectional=True, batch_first=True)\n",
        "        self.lstm.flatten_parameters()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size, output_size),\n",
        "            nn.LogSoftmax(dim=2),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(inputs)\n",
        "\n",
        "        output = out[:, :, :self.hidden_size] + out[:, :, self.hidden_size:]\n",
        "\n",
        "        out = self.mlp(output)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Postagger_adaptive(nn.Module):\n",
        "    def __init__(self, seq_length, emb_dim, hidden_size, output_size):\n",
        "        super(Postagger_adaptive, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.seq_length = seq_length\n",
        "        self.lstm = nn.LSTM(emb_dim, self.hidden_size, 1, bidirectional=True, batch_first=True)\n",
        "        self.lstm.flatten_parameters()\n",
        "        self.out = nn.AdaptiveLogSoftmaxWithLoss(hidden_size, output_size, cutoffs=[round(output_size/5),2*round(output_size/5)], div_value=4)\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(inputs)\n",
        "\n",
        "        output = out[:, :, :self.hidden_size] + out[:, :, self.hidden_size:]\n",
        "        output = output.contiguous().view(output.shape[0]*output.shape[1], output.shape[2])\n",
        "        targets = targets.view(targets.shape[0]*targets.shape[1])\n",
        "\n",
        "        return self.out(output, targets)\n",
        "\n",
        "    def validation(self, inputs, targets):\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(inputs)\n",
        "\n",
        "        output = out[:, :, :self.hidden_size] + out[:, :, self.hidden_size:]\n",
        "        output = output.contiguous().view(output.shape[0] * output.shape[1], -1)\n",
        "        targets = targets.view(targets.shape[0]*targets.shape[1])\n",
        "\n",
        "        prediction = self.out.predict(output)\n",
        "        _, loss = self.out(output, targets)\n",
        "\n",
        "        return prediction, float(loss.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AJYJSynaVfQb"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(\n",
        "    description='Conditional Text Generation: Train Discriminator'\n",
        ")\n",
        "\n",
        "parser.add_argument('--maxepoch', default=30, help='maximum iteration (default=1000)')\n",
        "parser.add_argument('--run', default=1, help='starting epoch (default=1)')\n",
        "parser.add_argument('--save', default=False, action='store_true', help='whether to save model or not')\n",
        "parser.add_argument('--load', default=False, action='store_true', help='whether to load model or not')\n",
        "parser.add_argument('--lang', default='en', help='choose which language for word embedding')\n",
        "parser.add_argument('--model', default='lstm', help='choose which mimick model')\n",
        "parser.add_argument('--lr', default=0.1, help='learning rate')\n",
        "parser.add_argument('--charlen', default=20, help='maximum length')\n",
        "parser.add_argument('--charembdim', default=300)\n",
        "parser.add_argument('--embedding', default='polyglot')\n",
        "parser.add_argument('--local', default=False, action='store_true')\n",
        "parser.add_argument('--loss_fn', default='mse')\n",
        "parser.add_argument('--dropout', default=0)\n",
        "parser.add_argument('--bsize', default=64)\n",
        "parser.add_argument('--epoch', default=0)\n",
        "parser.add_argument('--asc', default=False, action='store_true')\n",
        "parser.add_argument('--quiet', default=False, action='store_true')\n",
        "parser.add_argument('--init_weight', default=False, action='store_true')\n",
        "parser.add_argument('--shuffle', default=False, action='store_true')\n",
        "parser.add_argument('--nesterov', default=False, action='store_true')\n",
        "parser.add_argument('--loss_reduction', default=False, action='store_true')\n",
        "parser.add_argument('--num_feature', default=50)\n",
        "parser.add_argument('--weight_decay', default=0)\n",
        "parser.add_argument('--momentum', default=0)\n",
        "parser.add_argument('--multiplier', default=1)\n",
        "parser.add_argument('--classif', default=200)\n",
        "parser.add_argument('--neighbor', default=5)\n",
        "parser.add_argument('--seq_len', default=5)\n",
        "\n",
        "max_epoch = 50\n",
        "run = 1\n",
        "save = True\n",
        "load = False\n",
        "lang = \"en\"\n",
        "model = \"lstm\"\n",
        "lr = 0.1\n",
        "charlen = 20\n",
        "charembdim = 300\n",
        "embedding = \"word2vec\"\n",
        "local = True\n",
        "loss_fn = \"mse\"\n",
        "dropout = 0\n",
        "bsize = 64\n",
        "epoch = 0\n",
        "asc = False\n",
        "quiet = False\n",
        "init_weight = False\n",
        "shuffle = False\n",
        "nesterov = False\n",
        "loss_reduction = False\n",
        "num_feature = 50\n",
        "weight_decay = 0\n",
        "momentum = 0\n",
        "multiplier = 1\n",
        "classif = 200\n",
        "neighbor = 5\n",
        "seq_len = 5\n",
        "\n",
        "argmnt = [\n",
        "    '--maxepoch', f'{max_epoch}',\n",
        "    '--run', f'{run}',\n",
        "    '--lang', f'{lang}',\n",
        "    '--model', f'{model}',\n",
        "    '--lr', f'{lr}',\n",
        "    '--charlen', f'{charlen}',\n",
        "    '--charembdim', f'{charembdim}',\n",
        "    '--embedding', f'{embedding}',\n",
        "    '--loss_fn', f'{loss_fn}',\n",
        "    '--dropout', f'{dropout}',\n",
        "    '--bsize', f'{bsize}',\n",
        "    '--epoch', f'{epoch}',\n",
        "    '--num_feature', f'{num_feature}',\n",
        "    '--weight_decay', f'{weight_decay}',\n",
        "    '--momentum', f'{momentum}',\n",
        "    '--multiplier', f'{multiplier}',\n",
        "    '--classif', f'{classif}',\n",
        "    '--neighbor', f'{neighbor}',\n",
        "    '--seq_len', f'{seq_len}',\n",
        "]\n",
        "if local: argmnt += ['--local']\n",
        "if asc: argmnt += ['--asc']\n",
        "if quiet: argmnt += ['--quiet']\n",
        "if init_weight: argmnt += ['--init_weight']\n",
        "if shuffle: argmnt += ['--shuffle']\n",
        "if nesterov: argmnt += ['--nesterov']\n",
        "if loss_reduction: argmnt += ['--loss_reduction']\n",
        "if save: argmnt += ['--save']\n",
        "if load: argmnt += ['--load']\n",
        "\n",
        "args = parser.parse_args(argmnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "62d3138bc3084a1790e757d3ad1fbe2b",
            "33a537f97a5640bab9860dad164e5855",
            "5ade38ae9b754f26ac46a2902e5bd627",
            "9b08b84e50c743fdb3455dd197f3ca94",
            "cae6780cb40b4679b3aa3fd21b7aa139",
            "b693a720762a48ab882b91ba2c8b712e",
            "34bdb692e1314f68816dee915ebc01f1",
            "2898160943af424ab175a652a8817445",
            "8751db9f9f5e488e8341c458fac3462d",
            "74b1c3c5d5094abda57a3b593d85d959",
            "ba42b43e624b4b4786ddfb87276ed1ce",
            "bec65bf980554f398f44ebd7105fb6b8",
            "4fa9b04ae47a406d912b43f1ad0f77e1",
            "3da6e444e92d48a894fa13206c6cb47e",
            "afc1fec1b7aa49d4a5ef91e7b2e0ca71",
            "4f12c07a63c34ccf8ce4dc40692a6d2c"
          ]
        },
        "id": "7PY7jGEYRKUE",
        "outputId": "7365a9ff-aacb-43e2-941f-c7d075176b9d"
      },
      "outputs": [],
      "source": [
        "#@title POStag Train\n",
        "import shutil\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "        m.weight.data.fill_(0.01)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "# *Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "cloud_dir = '/content/gdrive/My Drive/train_dropout/'\n",
        "saved_model_path = 'trained_model_%s_%s_%s_%s' % (args.lang, args.model, args.embedding, args.loss_fn)\n",
        "saved_postag_path = 'trained_model_%s_%s_%s_postag' % (args.lang, args.model, args.loss_fn)\n",
        "logger_dir = '%s/logs/run%s/' % (saved_postag_path, args.run)\n",
        "logger_val_dir = '%s/logs/val-run%s/' % (saved_postag_path, args.run)\n",
        "logger_val_cosine_dir = '%s/logs/val-cosine-run%s/' % (saved_postag_path, args.run)\n",
        "\n",
        "if not args.local:\n",
        "    # logger_dir = cloud_dir + logger_dir\n",
        "    saved_model_path = cloud_dir + saved_model_path\n",
        "    saved_postag_path = cloud_dir + saved_postag_path\n",
        "\n",
        "\n",
        "wab_name = f\"{args.model}{args.embedding}_{curr_dt.year-2000}{curr_dt.month:02}{curr_dt.day:02}{curr_dt.hour:02}{curr_dt.minute:02}_POStag\"\n",
        "wandb.init(\n",
        "    # Set the project where this run will be logged\n",
        "    project=\"Mimick\",\n",
        "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "    name=wab_name,\n",
        "    config={\n",
        "        \"run\":int(args.run),\n",
        "        \"char_emb_dim\":int(args.charembdim),\n",
        "        \"char_max_len\":int(args.charlen),\n",
        "        \"char_emb_ascii\":args.asc,\n",
        "        \"shuffle_dataset\":args.shuffle,\n",
        "        \"neighbor\":int(args.neighbor),\n",
        "        \"validation_split\":.8,\n",
        "        \"batch_size\":int(args.bsize),\n",
        "        \"max_epoch\":int(args.maxepoch),\n",
        "        \"learning_rate\":float(args.lr),\n",
        "        \"weight_decay\":float(args.weight_decay),\n",
        "        \"momentum\":float(args.momentum),\n",
        "        \"multiplier\":float(args.multiplier),\n",
        "        \"classif\":int(args.classif),\n",
        "        \"model\":args.model,\n",
        "        \"loss_fn\":args.loss_fn,\n",
        "        \"loss_reduction\":args.loss_reduction,\n",
        "        \"seq_len\":int(args.seq_len)\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# *Parameters\n",
        "char_emb_dim = int(args.charembdim)\n",
        "char_max_len = int(args.charlen)\n",
        "random_seed = 64\n",
        "shuffle_dataset = args.shuffle\n",
        "validation_split = .8\n",
        "neighbor = int(args.neighbor)\n",
        "seq_len = int(args.seq_len)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# *Hyperparameter\n",
        "batch_size = int(args.bsize)\n",
        "val_batch_size = 64\n",
        "max_epoch = int(args.maxepoch)\n",
        "learning_rate = float(args.lr)\n",
        "weight_decay = float(args.weight_decay)\n",
        "momentum = float(args.momentum)\n",
        "multiplier = float(args.multiplier)\n",
        "classif = int(args.classif)\n",
        "\n",
        "char_embed = Char_embedding(char_emb_dim, char_max_len, asc=args.asc, random=True, device=device)\n",
        "# char_embed.embed.load_state_dict(torch.load('%s/charembed.pth' % saved_model_path))\n",
        "# char_embed.embed.eval()\n",
        "dataset = Postag(char_embed)\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "\n",
        "if shuffle_dataset:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "#* Creating PT data samplers and loaders:\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "np.random.shuffle(train_indices)\n",
        "np.random.shuffle(val_indices)\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size,\n",
        "                                sampler=train_sampler)\n",
        "validation_loader = DataLoader(dataset, batch_size=val_batch_size,\n",
        "                                sampler=valid_sampler)\n",
        "\n",
        "#* Initializing model\n",
        "word_embedding = Word_embedding(lang=args.lang, embedding=args.embedding)\n",
        "emb_dim = word_embedding.emb_dim\n",
        "\n",
        "if args.model == 'lstm':\n",
        "    model = mimick(char_embed.embed, char_embed.char_emb_dim, emb_dim, int(args.num_feature))\n",
        "elif args.model == 'cnn2':\n",
        "    model = mimick_cnn2(\n",
        "        embedding=char_embed.embed,\n",
        "        char_max_len=char_embed.char_max_len,\n",
        "        char_emb_dim=char_embed.char_emb_dim,\n",
        "        emb_dim=emb_dim,\n",
        "        num_feature=int(args.num_feature),\n",
        "        random=False, asc=args.asc)\n",
        "elif args.model == 'cnn':\n",
        "    model = mimick_cnn(\n",
        "        embedding=char_embed.embed,\n",
        "        char_max_len=char_embed.char_max_len,\n",
        "        char_emb_dim=char_embed.char_emb_dim,\n",
        "        emb_dim=emb_dim,\n",
        "        num_feature=int(args.num_feature),\n",
        "        random=False, asc=args.asc)\n",
        "elif args.model == 'cnn3':\n",
        "    model = mimick_cnn3(\n",
        "        embedding=char_embed.embed,\n",
        "        char_max_len=char_embed.char_max_len,\n",
        "        char_emb_dim=char_embed.char_emb_dim,\n",
        "        emb_dim=emb_dim,\n",
        "        num_feature=int(args.num_feature),\n",
        "        mtp=multiplier,\n",
        "        random=False, asc=args.asc)\n",
        "elif args.model == 'cnn4':\n",
        "    model = mimick_cnn4(\n",
        "        embedding=char_embed.embed,\n",
        "        char_max_len=char_embed.char_max_len,\n",
        "        char_emb_dim=char_embed.char_emb_dim,\n",
        "        emb_dim=emb_dim,\n",
        "        num_feature=int(args.num_feature),\n",
        "        classif=classif,\n",
        "        random=False, asc=args.asc)\n",
        "else:\n",
        "    model = None\n",
        "\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load('%s/%s.pth' % (saved_model_path, args.model)))\n",
        "model.eval()\n",
        "\n",
        "postagger = Postagger_adaptive(seq_len, emb_dim, 20, len(dataset.tagset)).to(device)\n",
        "# postagger = Postagger(seq_len, emb_dim, 20, len(dataset.tagset)).to(device)\n",
        "\n",
        "if args.load:\n",
        "    postagger.load_state_dict(torch.load('%s/postag.pth' % (saved_postag_path)))\n",
        "\n",
        "optimizer = optim.SGD(postagger.parameters(), lr=learning_rate, momentum=momentum, nesterov=args.nesterov)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "if args.init_weight: postagger.apply(init_weights)\n",
        "step = 0\n",
        "\n",
        "#* Training\n",
        "for epoch in trange(int(args.epoch), max_epoch, total=max_epoch, initial=int(args.epoch)):\n",
        "    loss_item = 0.\n",
        "    for it, (X, y) in enumerate(train_loader):\n",
        "        postagger.zero_grad()\n",
        "        if args.model == 'lstm':\n",
        "            inputs = X.view(X.shape[0]*seq_len, X.shape[2]).to(device)\n",
        "        else:\n",
        "            inputs = X.view(X.shape[0]*seq_len, 1, -1).to(device)\n",
        "        w_embedding = Variable(model.forward(inputs).view(X.shape[0], seq_len, -1), requires_grad=True).to(device) # (batch x sent_length x word_emb_dim)\n",
        "        target = Variable(y).to(device)\n",
        "        # output = postagger.forward(w_embedding, target).permute(0, 2, 1)\n",
        "        output, loss = postagger.forward(w_embedding, target)\n",
        "\n",
        "        # loss = criterion(output, target)\n",
        "\n",
        "        # ##################\n",
        "        # Tensorboard\n",
        "        # ##################\n",
        "        # loss_item = loss.item() if not args.loss_reduction else loss.mean().item()\n",
        "        # info = {\n",
        "        #     'loss-Train-%s-postag-run%s' % (args.model, args.run) : loss_item,\n",
        "        # }\n",
        "\n",
        "        # step += 1\n",
        "        # if args.run != 0:\n",
        "        #     for tag, value in info.items():\n",
        "        #         logger.scalar_summary(tag, value, step)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss_item += loss.detach().cpu().item() if not args.loss_reduction else loss.detach().cpu().mean().item()\n",
        "\n",
        "    wandb.log({\"train_loss\": loss_item})\n",
        "\n",
        "    # if not args.local:\n",
        "    #     copy_tree(logger_dir, cloud_dir+logger_dir)\n",
        "\n",
        "    if not os.path.exists(saved_postag_path):\n",
        "        os.makedirs(saved_postag_path)\n",
        "\n",
        "    torch.save(postagger.state_dict(), '%s/postag.pth' % (saved_postag_path))\n",
        "    if not args.quiet: tqdm.write('%d | %.4f ' % (epoch, loss_item))\n",
        "\n",
        "    #* Validation\n",
        "    postagger.eval()\n",
        "    validation_loss = 0.\n",
        "    accuracy = 0.\n",
        "    for it, (X, y) in enumerate(validation_loader):\n",
        "        if args.model == 'lstm':\n",
        "            inputs = X.view(X.shape[0]*seq_len, X.shape[2]).to(device)\n",
        "        else:\n",
        "            inputs = X.view(X.shape[0]*seq_len, 1, -1).to(device)\n",
        "        w_embedding = Variable(model.forward(inputs).view(X.shape[0], seq_len, -1), requires_grad=False).to(device) # (batch x sent_length x word_emb_dim)\n",
        "        target = Variable(y).to(device)\n",
        "        # output = postagger.forward(w_embedding).permute(0, 2, 1)\n",
        "        output, validation_loss = postagger.validation(w_embedding, target)\n",
        "        # output_tag = postagger.predict(output.view(X.shape[0], seq_len))\n",
        "        output_tag = output.view(X.shape[0], seq_len)\n",
        "        correct = (output_tag == target).sum()/(len(val_indices)*seq_len)\n",
        "        accuracy += correct\n",
        "        # validation_loss += criterion(output, target)*X.shape[0]/(len(val_indices))\n",
        "        if not args.quiet:\n",
        "            if it == 0:\n",
        "                tag = output_tag[0]\n",
        "                # output_tag = postagger.predict(output.view(X.shape[0], seq_len)[0])\n",
        "                for i in range(len(X[0])):\n",
        "                    word_idx = X[0][i].numpy()\n",
        "                    word = char_embed.clean_idxs2word(word_idx)\n",
        "                    tg = dataset.tagset.idx2tag(int(tag[i].cpu()))\n",
        "                    tgt = dataset.tagset.idx2tag(int(y[0][i]))\n",
        "                    tqdm.write('(%s, %s) => %s' % (word, tgt, tg))\n",
        "    if not args.quiet: tqdm.write('accuracy = %.4f' % accuracy)\n",
        "\n",
        "    # info_val = {\n",
        "    #     'loss-Train-%s-postag-run%s' % (args.model, args.run) : validation_loss\n",
        "    # }\n",
        "\n",
        "    # if args.run != 0:\n",
        "    #     for tag, value in info_val.items():\n",
        "    #         logger_val.scalar_summary(tag, validation_loss, step)\n",
        "\n",
        "    if not args.quiet: tqdm.write('val_loss %.4f ' % validation_loss)\n",
        "\n",
        "    postagger.train()\n",
        "\n",
        "postagger.eval()\n",
        "\n",
        "accuracy = 0.\n",
        "for it, (X, y) in enumerate(validation_loader):\n",
        "    if args.model == 'lstm':\n",
        "        inputs = X.view(X.shape[0]*seq_len, X.shape[2]).to(device)\n",
        "    else:\n",
        "        inputs = X.view(X.shape[0]*seq_len, 1, -1).to(device)\n",
        "    w_embedding = Variable(model.forward(inputs).view(X.shape[0], 5, -1), requires_grad=False).to(device) # (batch x sent_length x word_emb_dim)\n",
        "    target = Variable(y).to(device)\n",
        "    # output = postagger.forward(w_embedding).permute(0, 2, 1)\n",
        "    output, _ = postagger.validation(w_embedding, target)\n",
        "    # output_tag = postagger.predict(output.view(X.shape[0], seq_len))\n",
        "    output_tag = output.view(X.shape[0], seq_len)\n",
        "    correct = float((output_tag == target).sum())/(len(val_indices)*seq_len)\n",
        "    accuracy += correct\n",
        "    if it <= 3:\n",
        "        tag = output_tag[0]\n",
        "        for i in range(len(X[0])):\n",
        "            word_idx = X[0][i].numpy()\n",
        "            word = char_embed.clean_idxs2word(word_idx)\n",
        "            tg = dataset.tagset.idx2tag(int(tag[i].cpu()))\n",
        "            tgt = dataset.tagset.idx2tag(int(y[0][i]))\n",
        "            tqdm.write('(%s, %s) => %s' % (word, tgt, tg))\n",
        "        tqdm.write('\\n')\n",
        "print('accuracy = %.4f' % accuracy)\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gnYt4MLabivB"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bdfeb8d82cb4987ba1319908b49bf85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f94e40ea772a4843b312661b5ec2e35a",
              "IPY_MODEL_4b2b37ebdf524cfa9b76979cadba6248"
            ],
            "layout": "IPY_MODEL_e444da69d34548ed9352cf35c74bcc9f"
          }
        },
        "2898160943af424ab175a652a8817445": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "314d6af552c3451db595d759ec78ba4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33a537f97a5640bab9860dad164e5855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae6780cb40b4679b3aa3fd21b7aa139",
            "placeholder": "​",
            "style": "IPY_MODEL_b693a720762a48ab882b91ba2c8b712e",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "34bdb692e1314f68816dee915ebc01f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3da6e444e92d48a894fa13206c6cb47e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b2b37ebdf524cfa9b76979cadba6248": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d50f7536c7cf482fb09e9e7803f9c836",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f000672be1f4f57a340527bb738e089",
            "value": 1
          }
        },
        "4f12c07a63c34ccf8ce4dc40692a6d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fa9b04ae47a406d912b43f1ad0f77e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ade38ae9b754f26ac46a2902e5bd627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34bdb692e1314f68816dee915ebc01f1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2898160943af424ab175a652a8817445",
            "value": 1
          }
        },
        "62d3138bc3084a1790e757d3ad1fbe2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33a537f97a5640bab9860dad164e5855",
              "IPY_MODEL_5ade38ae9b754f26ac46a2902e5bd627"
            ],
            "layout": "IPY_MODEL_9b08b84e50c743fdb3455dd197f3ca94"
          }
        },
        "74b1c3c5d5094abda57a3b593d85d959": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fa9b04ae47a406d912b43f1ad0f77e1",
            "placeholder": "​",
            "style": "IPY_MODEL_3da6e444e92d48a894fa13206c6cb47e",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "8751db9f9f5e488e8341c458fac3462d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74b1c3c5d5094abda57a3b593d85d959",
              "IPY_MODEL_ba42b43e624b4b4786ddfb87276ed1ce"
            ],
            "layout": "IPY_MODEL_bec65bf980554f398f44ebd7105fb6b8"
          }
        },
        "9b08b84e50c743fdb3455dd197f3ca94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f000672be1f4f57a340527bb738e089": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2785f556f684fdc8a9da40c9d0041dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afc1fec1b7aa49d4a5ef91e7b2e0ca71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b693a720762a48ab882b91ba2c8b712e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba42b43e624b4b4786ddfb87276ed1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afc1fec1b7aa49d4a5ef91e7b2e0ca71",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f12c07a63c34ccf8ce4dc40692a6d2c",
            "value": 1
          }
        },
        "bec65bf980554f398f44ebd7105fb6b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cae6780cb40b4679b3aa3fd21b7aa139": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d50f7536c7cf482fb09e9e7803f9c836": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e444da69d34548ed9352cf35c74bcc9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f94e40ea772a4843b312661b5ec2e35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_314d6af552c3451db595d759ec78ba4b",
            "placeholder": "​",
            "style": "IPY_MODEL_a2785f556f684fdc8a9da40c9d0041dd",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
